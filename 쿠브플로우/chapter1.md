## 1.2 머신러닝 기초
- 주어진 데이터를 통해 학습
  - 지도학습: Labeling O
    - classification, regression 분석
  - 비지도학습: Labeling X
    - clustering, dimentionality reduction
- 데이터가 주어지지 않을 수 있음
  - 강화학습

### 1.2.1 선형회귀
> 데이터 간의 관계를 수학적으로 설명

- hypothesis: 결과값을 예측하게 해줌
- cost: 가설의 정확도를 판단하는 기준, 손실

최종 목표: 비용을 최소화하는 가설을 계산해 내는 것
- hypothesis 함수
  - 회귀 분석은 연속적인 자료 구조를 분석할 때 사용
  - Ex) 공부시간(x), 시험점수(y) 간의 관계를 나타내는 가장 적합한 방정식을 찾는 것이 선형 회귀 문제
    - hypothesis 함수: X, Y 사이의 관계를 나타내는 함수
      - $H(x) = Wx + b$

선형 회귀 분석은 주어진 데이터를 가장 잘 만족하는 W와 b를 찾는 것

그렇다면 비용함수를 사용해서 어떤 함수가 가장 적합한지 판단할 수 있음

- 비용함수
  - 정량 분석을 위해 가설 함수의 값과 실제 데이터 값 사이의 오차를 사용 -> 오차가 가장 작은 가설 함수가 가장 적합한 함수
  - $H(x) - y$ : 함수의 값에서 실제 데이터의 값을 빼줌
  - 모든 X에 대한 오차를 더해주어 전체 오차를 가설 함수별로 비교함
    - 오차가 양수와 음수가 섞여있을 때 오차의 크기가 줄어드는 문제 발생
    - 오차의 정도를 비교하려면 모두 양수로 만들어야 하므로 오차의 제곱을 더해줌
  - $cost = \frac{1}{m} \sum_{i=1}^m (H(x^{(i)}) - y^{(i)})^2$
    - m개 존재하는 모든 X에 대해 오차의 제곱을 더해주고 이것을 다시 m으로 나누어 평균 오차를 구함
  - $cost(W,b) = \frac{1}{m} \sum_{i=1}^m (H(x^{(i)}) - y^{(i)})^2$

- 비용 최소화 알고리즘(Gradient Descent)
  - 간소화한 가설 함수: $H(x) = Wx$
  - 비용함수를 W에 대한 식으로 바꾸어 표현해보면, (비용을 최소화하는 W의 해를 구하는 문제)
    - $cost(W) = \frac{1}{m} \sum_{i=1}^m (Wx^{(i)} - y^{(i)})^2$
  - 접선의 기울기가 0일 때의 해를 구하는 알고리즘: 경사하강법(Gradient Descent)
    - $W_1 := W_0-\alpha \frac{\partial}{\partial W} cost(W_0)$
      - 첫번째 임의의 W0를 선택하고 이에 대해 위의 계산을 수행해서 다음번 W1을 찾는 것
      - $\frac{\partial}{\partial W} cost(W_0)$ 은 W0지점의 비용함수를 W에 대해 편미분한 값(=기울기)에 일정한 계수$\alpha$를 곱하고 이를 W0에서 빼줌
        - $\alpha$: learning rate
        - 기울기에 음수를 곱해주기 때문에 계산을 수행할수록 기울기의 절대값이 낮은 쪽으로 점점 수렴하게 됨, 부호가 양수라면 수렴이 아니라 발산함

- 차원의 확장(Multi variable linear regression)
  - 데이터 차원이 늘어나면 어떻게 해야될까?
  - Ex) X1(quiz1), X2(quiz2), X3(mid term), Y(final) 
