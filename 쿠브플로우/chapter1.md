## 1.2 머신러닝 기초
- 주어진 데이터를 통해 학습
  - 지도학습: Labeling O
    - classification, regression 분석
  - 비지도학습: Labeling X
    - clustering, dimentionality reduction
- 데이터가 주어지지 않을 수 있음
  - 강화학습

### 1.2.1 선형회귀
> 데이터 간의 관계를 수학적으로 설명

- hypothesis: 결과값을 예측하게 해줌
- cost: 가설의 정확도를 판단하는 기준, 손실

최종 목표: 비용을 최소화하는 가설을 계산해 내는 것
- hypothesis 함수
  - 회귀 분석은 연속적인 자료 구조를 분석할 때 사용
  - Ex) 공부시간(x), 시험점수(y) 간의 관계를 나타내는 가장 적합한 방정식을 찾는 것이 선형 회귀 문제
    - hypothesis 함수: X, Y 사이의 관계를 나타내는 함수
      - $H(x) = Wx + b$

선형 회귀 분석은 주어진 데이터를 가장 잘 만족하는 W와 b를 찾는 것

그렇다면 비용함수를 사용해서 어떤 함수가 가장 적합한지 판단할 수 있음

- 비용함수
  - 정량 분석을 위해 가설 함수의 값과 실제 데이터 값 사이의 오차를 사용 -> 오차가 가장 작은 가설 함수가 가장 적합한 함수
  - $H(x) - y$ : 함수의 값에서 실제 데이터의 값을 빼줌
  - 모든 X에 대한 오차를 더해주어 전체 오차를 가설 함수별로 비교함
    - 오차가 양수와 음수가 섞여있을 때 오차의 크기가 줄어드는 문제 발생
    - 오차의 정도를 비교하려면 모두 양수로 만들어야 하므로 오차의 제곱을 더해줌
  - $cost = \frac{1}{m} \sum_{i=1}^m (H(x^{(i)}) - y^{(i)})^2$
    - m개 존재하는 모든 X에 대해 오차의 제곱을 더해주고 이것을 다시 m으로 나누어 평균 오차를 구함
  - $cost(W,b) = \frac{1}{m} \sum_{i=1}^m (H(x^{(i)}) - y^{(i)})^2$

- 비용 최소화 알고리즘(Gradient Descent)
  - 간소화한 가설 함수: $H(x) = Wx$
  - 비용함수를 W에 대한 식으로 바꾸어 표현해보면, (비용을 최소화하는 W의 해를 구하는 문제)
    - $cost(W) = \frac{1}{m} \sum_{i=1}^m (Wx^{(i)} - y^{(i)})^2$
  - 접선의 기울기가 0일 때의 해를 구하는 알고리즘: 경사하강법(Gradient Descent)
    - $W_1 := W_0-\alpha \frac{\partial}{\partial W} cost(W_0)$
      - 첫번째 임의의 W0를 선택하고 이에 대해 위의 계산을 수행해서 다음번 W1을 찾는 것
      - $\frac{\partial}{\partial W} cost(W_0)$ 은 W0지점의 비용함수를 W에 대해 편미분한 값(=기울기)에 일정한 계수$\alpha$를 곱하고 이를 W0에서 빼줌
        - $\alpha$: learning rate
        - 기울기에 음수를 곱해주기 때문에 계산을 수행할수록 기울기의 절대값이 낮은 쪽으로 점점 수렴하게 됨, 부호가 양수라면 수렴이 아니라 발산함

### 1.2.2 차원의 확장(Multi variable linear regression)
> 데이터 차원이 늘어나면 어떻게 해야될까?
  
- Ex) X1(quiz1), X2(quiz2), X3(mid term), Y(final) 
  - 3종류의 입력값을 통해 출력값(Y)
  - 가설함수: $H(x_1, x_2, x_3) = w_1x_1 + w_2x_2 + w_3x_3 + b$
  - 행렬로 정리하면, $X = (x_1 x_2 x_3)$, $W=(w_1 w_2 w_3)^T$
  - 즉, $(x_1 x_2 x_3) \cdot (w_1 w_2 w_3)^T = x_1w_1 + x_2w_2 + x_3w_3$
  - 최종적으로 $H(x) = XW$
 
- 비용함수 및 경사하강법
  - $cost(W) = \frac{1}{m} \sum(WX-y)^2$
  - $W_1 := W_0-\alpha \frac{\partial}{\partial W} cost(W_0)$
    - W에 해당하는 값이 n개가 있지만 편미분을 하므로 관계 없음

### 1.2.3 로지스틱 회귀(Logistic Regression)
> 선형 회귀는 결과값이 연속적인 형태일 때 사용가능, 비연속적일 때는 적합한 방법이 아님(Ex. 사진이 개인가 고양이인가 판단하는 것)
>
> 로지스틱 회귀는 이처럼 비연속적이고, 둘 중에 하나를 선택하는 binary classification 문제에 적합


- 달라진 가설 함수(sigmoid 함수): $H(X)= \frac{1}{1+e^{-W^TX}}$
  - sigmoid 함수: 결과값이 0과 1사이
- 달라진 비용 함수: $cost(W) = -\frac{1}{m} \sum(ylog(H(x)) + (1-y)log(1-H(x)))$

### 1.2.4 소프트맥스 회귀(Softmax Regression, multi-class classfication)
> 로지스틱 회귀는 2개중 하나를 고르는 거라면, 소프트맥스 회귀는 여러가지 결과값 중 하나를 고르는 문제에 적용

- 소프트맥스 회귀는 개념상 로지스틱 회귀를 여러번 반복하면 수행할 수 있음
  - 개가 아닌지 판단 -> 고양이인지 판단 -> 원숭이인지 판단을 계속 반복하여 적합한 분류 선택
- 수학적으로는 소프트맥스 함수를 사용
  - 입력받은 값에 대해 출력으로 항상 0~1 사이의 값으로 정규화함
  - 출력 값들의 총합은 항상 1

### 1.3.1 learning rate
> 한 번 학습 후 다음 번 스텝을 어느 정도의 보폭으로 움직일지 결정하는 것

- 학습률이 중요한 이유: 최적화를 해나가는 과정에서 효율성에 매우 큰 영향을 줌
  - 학습률이 너무 작으면 적합한 해를 찾는데 너무 많은 시간 소모
  - 반대로 너무 크면 적합한 해를 찾지 못하거나 발산할 수 있음

- 각 그래프에 따라 적합한 학습률이 있음, 처음부터 외부에서 주어지는 요소이므로 hyperparameter라고 함
- 시행착오로 찾아가거나, 하이퍼 파라미터 최적화 튜닝 알고리즘, 방법론이 개발되고 있음 -> kubeflow의 katib를 통해 사용가능

### 1.3.2 batch normalization
> 데이터의 구조와 관련이 있음, 예를들어 X1의 데이터범위는 천단위, X2는 일의 단위라면 X2에 대해서 학습률이 너무 크기때문에 발산하게 됨
>
> 즉, 학습이 잘되게 하기 위해 데이터의 분포를 적당하게 조절해주는 것을 배치 정규화라고 함

### 1.3.3 overfitting
> 모델이 너무 잘 맞게 되는 것, 현재 갖고 있는 데이터에만 너무 잘 맞는 모델이 생성되면 새로운 데이터를 이용해 추론할 때 모델의 성능에 문제가 있을 수 있음

- overfitting, underfitting이 아닌 appropriate 모델을 원함
- 과적합을 해결하기 위한 방법
  - 데이터의 양을 늘림. 다양해질수록 더 많은 범위를 포용할 수 있고 적합성이 높아짐
  - validation 데이터를 분리함, 모든 데이터를 학습에만 사용하는 것이 아닌 검증에 사용할 데이터를 따로 마련
    - 보통, training / validatio / test 세가지 set으로 분류하고 비율을 6:2:2 정도
  - feature의 개수를 줄임, feature의 개수가 많을수록 복잡한 자료를 잘 학습할 수 있으나 또한 동시에 과적합 확률 up
    - feature: 특정한 성질만 추출하는 것
   
### 1.3.4 deep learning
> 머신러닝의 계산 단계를 인간의 뇌 구조인 뉴럴 넷을 본따서 네트워크로 구현한 것
