# 알아두면 좋을 몇 가지 주의사항
- **쿼리 타임아웃**: 쿼리 실행 시간을 제한 
  - 응답 지연으로 인한 재시도는 서버 부하를 더욱 가중시킴 -> 앞선 요청을 아직 처리 중인 상황에서 새로운 요청이 유입되기 때문
  - 쿼리 타임아웃을 걸어두면 이전 요청이 여전히 처리중인 상태가 아니므로 동시 요청 수의 폭증을 막을 수 있음

- **상태 변경 기능은 복제 DB에서 조회하지 않기**
  - 주-부 DB 구조에서는 변경은 main DB, 조회는 복제 DB를 사용함
  - 모든 select 쿼리를 복제 db에서 하는 경우가 있는데,
    - 주 db와 복제 db는 순간적으로 데이터가 불일치할 수 있음
      - 네트워크를 통해 복제 db에 데이터를 전달하고 복제 db는 자체 데이터에 변경 내용을 반영하기 까지 시간이 소요됨
    - 1) 복제 과정중에 select 조회를 하게 되면, 잘못된 데이터를 조회할 수 있게됨
    - 2) 트랜잭션 문제
      - 주db와 복제db 간 데이터 복제는 트랜잭션 커밋 시점에 이뤄지기 때문에, 데이터 불일치 발생
      - 회원가입, 변경, 등록, 삭제와 같이 INSERT, UPDATE, DELETE 쿼리를 실행하는 기능에서 변경 대상 데이터를 조회할 떄, 주db에서 select 쿼리를 실행해야 불일치 문제로 인한 오류가 방지됨

- **배치 쿼리 실행 시간 증가**
  - 배치 프로그램은 데이터를 일괄로 조회, 집계, 생성하는 작업 수행
  - 한번에 조회하고 집계하는 데이터가 많아질수록 일괄 처리용 쿼리의 실행 시간도 함께 증가하게 되는데, 그만큼의 메모리도 많이 점유하게 됨
  - 해결책
    - 배치에서 사용하는 쿼리의 실행 시간을 지속적으로 추적해서 문제되는 쿼리를 감지하고 원인을 찾아 해결하자
    - DB 장비를 스펙업할 수도 있지만, 커버링 인덱스를 활용해보거나 데이터를 일정 크기로 나눠서 처리하자

- **타입이 다른 컬럼 조인 주의**
  ```sql
  select u.userId, u.name, p.*
  from user u, push p
  where u.userId = 145
    and u.userId = p.receiverId
    and p.recieverType = 'MEMBER'
  order by p.id desc
  limit 100;
  ```
  - 먼저 user 테이블에서 userId 기준으로 비교, push 테이블의 인덱스가 설정된 receiverId 칼럼과 조인하므로 인덱스를 사용하는 것처럼 보임
  - 실제로는 인덱스를 활용하지 못할 수 있음, 두 칼럼의 타입이 user.userId의 타입은 integer, push.receiverId는 varchar 으로 다르기 때문임
    - 두 컬럼을 비교하는 과정에서 DB는 receiverId 컬럼을 integer 타입으로 매번 형변환을 하면서, receiverId 인덱스를 온전히 활용하지 못함
    - 그리고, 인덱스만 스캔하더라도 push 테이블의 데이터가 많다면 이 변환 작업으로 인해 쿼리 실행시간이 길어짐
  ```sql
  select u.userId, u.name, p.*
  from user u, push p
  where u.userId = 145
    and cast(u.userId as char character set utf8mb4) collate 'utf8mb4_unicode_ci' = p.receiverId
    and p.erceiverType = 'MEMBER'
  order by p.id desc
  limit 100;
  ```
  - MySQL에서 타입을 변환해 두 컬럼의 타입을 일치시키고 비교시키도록 변경해 쿼리 실행 중 발생하는 불필요한 타입 변환을 줄일 수 있게 됨

- **테이블 변경은 신중하게**
  - DB의 테이블 변경방식 때문에 주의해야함
  - Ex) MySQL에서 테이블을 변경할 때, 새 테이블을 생성하고 원본 테이블의 데이터를 복사하고 복사가 완료되면 새 테이블로 대체함
  - 이 복사 과정동안, UPDATE, INSERT, DELETE 같은 DML 작업을 허용하지 않아 복사시간만큼 서비스가 중지됨

  - **DB 최대 연결 개수**
    - Ex) API 서버 3대 / 트래픽이 증가하고 있어 수평확장 필요 / DB 서버 CPU 사용률은 20% 수준
    - 새로 API 서버를 추가했는데도 DB 커넥션 생성에 실패한다면?
      - DB 서버 자원은 여유가 있지만 API 서버에서 DB연결에 실패한다면 DB에 설정된 최대 연결 개수를 확인해보자
      - DB의 최대 연결 개수가 100, API 서버의 커넥션 풀 개수가 30개로 총, 4 * 30 = 120개의 커넥션 수
      - 하지만 DB는 100이 최대이므로 20개는 커넥션을 얻지못하고 연결 실패가 발생하므로 최대 연결 개수를 늘려서 해결
    - 주의사항
      - DB 서버의 CPU 사용률이 70% 이라면, 연결 개수를 늘리면 안됨
      - 연결 수가 많아질수록 DB 부하가 증가하고 성능 저하 발생
      - 먼저 캐시서버를 구성하거나 쿼리 튜닝으로 DB 부하를 낮추고 필요시 연결 개수를 늘리자
